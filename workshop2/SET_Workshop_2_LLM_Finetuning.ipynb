{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# LLM Fundamentals: A Brief Introduction\n",
        "# Recurrent Neural Networks Basics\n",
        "\n",
        "Language models started with RNNs - neural networks that process text one word at a time. Think of RNN as a reader that remembers what it read before. However, basic RNNs had memory problems with long texts, so better versions were created.\n",
        "\n",
        "LSTM networks solved the memory problem by adding a system to remember important things and forget unimportant ones. GRU networks came later as a simpler version that works almost as well. These improvements made it possible to process longer texts effectively.\n",
        "\n",
        "# Working with Sequences\n",
        "Language models handle different sequence tasks. The simplest is predicting the next word, like your phone's keyboard suggestions. More complex tasks include translation, where text goes from one language to another, or summarization, where long text becomes short.\n",
        "These tasks use an encoder-decoder structure. The encoder reads and understands the input, while the decoder creates the output. Modern models use attention - a technique that helps focus on relevant parts of the input, much like how humans focus on important parts of a sentence.\n",
        "\n",
        "# Attention Mechanism\n",
        "Core Idea\n",
        "Attention lets a model focus on important parts of input when creating output. Like when you translate \"The cat sat on the mat\" - to translate 'cat' you mainly need to focus on the word 'cat', not other words. Attention does exactly this - it gives different importance weights to different input words.\n",
        "How It Works\n",
        "The process uses Query, Key, and Value - like searching a library:\n",
        "\n",
        "Query: What you're looking for\n",
        "Keys: Labels on information\n",
        "Values: The actual information\n",
        "\n",
        "The model matches Query with Keys to figure out which Values are important right now. It then combines the important parts to make its output.\n",
        "Why It's Important\n",
        "Before attention, models had to remember everything as they processed text one word at a time. With attention, models can directly look at any part of the input when needed - like having the whole text visible at once instead of reading through a keyhole.\n",
        "This is why modern language models work so well - they can easily connect related words even if they're far apart in the text.\n",
        "\n",
        "# Training\n",
        "Training involves showing the model examples and letting it learn from mistakes. The process needs lots of data and computing power. Models learn by predicting next words and checking if they're right. This simple task actually teaches them grammar, facts, and reasoning.\n",
        "\n",
        "Challenges include managing computer memory, keeping training stable, and making sure the model learns useful patterns. Modern LLMs use these same basic principles but at a much larger scale, which gives them their impressive capabilities.\n",
        "# Training vs Fine-tuning Language Models\n",
        "Pre-training\n",
        "Pre-training is like giving the model general education. The model learns language by reading massive amounts of text from the internet, books, and articles. During this phase, it learns:\n",
        "\n",
        "Basic language understanding\n",
        "Grammar and vocabulary\n",
        "General knowledge\n",
        "Basic reasoning abilities\n",
        "\n",
        "This process is extremely expensive. It requires:\n",
        "\n",
        "Months of training time\n",
        "Thousands of GPUs\n",
        "Millions of dollars\n",
        "Huge datasets (hundreds of billions of tokens)\n",
        "\n",
        "Fine-tuning\n",
        "Fine-tuning is like specialized training for a specific job. You take a pre-trained model and teach it specific skills or knowledge. It's much cheaper and faster than pre-training because:\n",
        "\n",
        "Uses much less data (hundreds to thousands of examples)\n",
        "Takes hours instead of months\n",
        "Can run on a few GPUs or even one\n",
        "Costs hundreds instead of millions of dollars\n",
        "\n",
        "Common fine-tuning scenarios:\n",
        "\n",
        "Teaching specific writing styles\n",
        "Training for customer service\n",
        "Learning company-specific knowledge\n",
        "Following specific formats or rules\n",
        "\n",
        "Key Differences\n",
        "Think of pre-training as building a brain, while fine-tuning is teaching new skills to that brain:\n",
        "\n",
        "Pre-training learns from scratch, fine-tuning adjusts existing knowledge\n",
        "Pre-training is general, fine-tuning is specific\n",
        "Pre-training needs massive resources, fine-tuning is lightweight\n",
        "Pre-training builds foundation, fine-tuning adds specialization\n",
        "\n",
        "When to Use What\n",
        "Use pre-training when:\n",
        "\n",
        "Building a new base model from scratch\n",
        "Need completely new language capabilities\n",
        "Have massive resources available\n",
        "\n",
        "\n",
        "**Use fine-tuning when:**\n",
        "\n",
        "Adapting existing model to specific tasks\n",
        "Need consistent output format\n",
        "Have limited resources\n",
        "Want to add domain expertise\n"
      ],
      "metadata": {
        "id": "4CD5V0pEhaWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation of libraries"
      ],
      "metadata": {
        "id": "_n1QirEslHa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy matplotlib pandas seaborn\n"
      ],
      "metadata": {
        "id": "GJA8MCOsi1OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "40G6oO3WmbWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "P6bDiZBWmdYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence_data(samples=1000, sequence_length=20):\n",
        "\n",
        "    x = np.linspace(0, 100, samples + sequence_length)\n",
        "    y_pure = np.sin(0.1 * x)\n",
        "    y_noisy = y_pure + np.random.normal(0, 0.1, len(x))\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x[:200], y_pure[:200], 'b-', label='Pure Signal')\n",
        "    plt.title('Original Sine Wave')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x[:200], y_noisy[:200], 'r.', label='Noisy Data')\n",
        "    plt.plot(x[:200], y_pure[:200], 'b-', alpha=0.3, label='Pure Signal')\n",
        "    plt.title('Training Data (with Noise)')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return y_noisy"
      ],
      "metadata": {
        "id": "OccfEhpZmhVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(data, seq_length):\n",
        "    \"\"\"Create input/output sequences\"\"\"\n",
        "    x, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x.append(data[i:(i + seq_length)])\n",
        "        y.append(data[i + seq_length])\n",
        "    return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "xvw1vRtympH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, output_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class LSTMPredictor(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, output_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "class GRUPredictor(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, output_size=1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "rp8Y3VMSmtyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, epochs=100):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item():.6f}')\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "sGMXofhPmzWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(model, X, y, title):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X).cpu().numpy()\n",
        "        actual = y.cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(actual[:100], 'b-', label='Actual', alpha=0.5)\n",
        "    plt.plot(predictions[:100], 'r-', label='Predicted', alpha=0.5)\n",
        "    plt.title(f'{title} - First 100 Predictions')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "l6evqV7em3fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating data...\")\n",
        "data = generate_sequence_data()\n",
        "sequence_length = 20\n",
        "X, y = create_sequences(data, sequence_length)\n",
        "\n",
        "# Split data and convert to tensors\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train).reshape(-1, sequence_length, 1).to(device)\n",
        "y_train = torch.FloatTensor(y_train).to(device)\n",
        "X_test = torch.FloatTensor(X_test).reshape(-1, sequence_length, 1).to(device)\n",
        "y_test = torch.FloatTensor(y_test).to(device)\n"
      ],
      "metadata": {
        "id": "bBtDDUcMm574"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nInitializing models...\")\n",
        "models = {\n",
        "    'RNN': SimpleRNN().to(device),\n",
        "    'LSTM': LSTMPredictor().to(device),\n",
        "    'GRU': GRUPredictor().to(device)\n",
        "}\n",
        "\n",
        "# Train all models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    losses = train_model(model, X_train, y_train)\n",
        "    results[name] = losses"
      ],
      "metadata": {
        "id": "r9JZ1B8Tm_X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for name, losses in results.items():\n",
        "    plt.plot(losses, label=name)\n",
        "plt.title('Training Loss Comparison')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uV689mqznV5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerating predictions...\")\n",
        "for name, model in models.items():\n",
        "    plot_predictions(model, X_test, y_test, name)\n",
        "\n",
        "# Calculate and print MSE for each model\n",
        "print(\"\\nModel Performance (MSE):\")\n",
        "criterion = nn.MSELoss()\n",
        "for name, model in models.items():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_predictions = model(X_test)\n",
        "        mse = criterion(test_predictions, y_test)\n",
        "        print(f\"{name} MSE: {mse.item():.6f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "id": "dfvCrWYfnaYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Instruction Fine-tuning\n",
        "Think of this as teaching the model to follow specific commands.\n",
        "Best Use Cases:\n",
        "\n",
        "Classification tasks\n",
        "Information extraction\n",
        "Structured output generation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Example Format:\n",
        "\n",
        "\n",
        "```\n",
        "    \"instruction\": \"Extract the company names from this text\",\n",
        "    \"input\": \"Apple and Microsoft announced a partnership today\",\n",
        "    \"output\": [\"Apple\", \"Microsoft\"]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Key Considerations:\n",
        "\n",
        "Clear, consistent instructions\n",
        "Well-defined output format\n",
        "Quality of instruction examples\n",
        "\n",
        "2. Chat Fine-tuning\n",
        "Perfect for conversational applications.\n",
        "Best Use Cases:\n",
        "\n",
        "Customer service bots\n",
        "Educational assistants\n",
        "Interactive agents\n",
        "\n",
        "Training Format:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What's your return policy?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Our standard return window is 30 days...\"},\n",
        "        {\"role\": \"user\", \"content\": \"What if the item is damaged?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"For damaged items, we offer immediate replacement...\"}\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Key Points:\n",
        "\n",
        "Conversation flow matters\n",
        "Context handling is crucial\n",
        "Role consistency is important\n",
        "\n",
        "3. Text Completion\n",
        "The simplest approach - completing or generating text.\n",
        "Best For:\n",
        "\n",
        "Code completion\n",
        "Content generation\n",
        "Text continuation\n",
        "\n",
        "Format:\n",
        "\n",
        "\n",
        "```\n",
        "{\n",
        "    \"prompt\": \"def calculate_fibonacci(n):\",\n",
        "    \"completion\": \"    if n <= 1:\\n        return n\\n    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\"\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ff4tYqmJpGnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "55JU-vDdzWqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example - BLOOM Fine-tuning"
      ],
      "metadata": {
        "id": "aoUqHKZGzNTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate torch wandb\n",
        "!pip install -q peft"
      ],
      "metadata": {
        "id": "F5qn1JZLzCsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8v0Um40AzXil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bigscience/bloom-560m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "HIuYyqs6zo9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZBORQSAzrP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    # Create labels for casual language modeling\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZfmannFzwvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names\n",
        ")\n",
        "test_tokenized = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "dD9RzFBDzxR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bloom-wiki-tuned\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_steps=500\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=test_tokenized\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "trainer.save_model()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "training_loss = [x[\"loss\"] for x in trainer.state.log_history if \"loss\" in x]\n",
        "plt.plot(training_loss)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_prompts = [\n",
        "    \"The history of Rome\",\n",
        "    \"Quantum mechanics is\",\n",
        "    \"The Industrial Revolution\"\n",
        "]\n",
        "\n",
        "print(\"\\nGeneration Examples:\")\n",
        "for prompt in test_prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    print(f\"Generated: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "r1dgohqC0jCX",
        "outputId": "b8f7ebd1-8812-428f-b0d8-4e9c671bc498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='9180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 500/9180 06:19 < 1:50:08, 1.31 it/s, Epoch 0.05/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.393900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.339500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.501900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}